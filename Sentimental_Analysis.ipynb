{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentimental_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtZswBz1dMhg",
        "colab_type": "code",
        "outputId": "4b3af566-7dab-4b14-8d16-8e5718ca5004",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIcYXPJUdiw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOuZPN6GdwZp",
        "colab_type": "code",
        "outputId": "702004c5-c772-4ca6-dd31-99dd75bdb0f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "!ls 'drive/My Drive/Hackathon/Hackathon/SentimentAnalysis'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "review_test.csv  review_train.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9586rlY4dznn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_train = pd.read_csv('drive/My Drive/Hackathon/Hackathon/SentimentAnalysis/review_train.csv')\n",
        "sent_test = pd.read_csv('drive/My Drive/Hackathon/Hackathon/SentimentAnalysis/review_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXeAZPe6d_A9",
        "colab_type": "code",
        "outputId": "de907fdc-274e-4fc9-b702-9aa7e0590666",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "sent_train.info()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 14825 entries, 0 to 14824\n",
            "Data columns (total 3 columns):\n",
            "Text         14825 non-null object\n",
            "Score        14825 non-null int64\n",
            "Sentiment    14825 non-null int64\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 347.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThCX30EWeKRb",
        "colab_type": "code",
        "outputId": "0aa4c154-195a-409b-c18b-866bd79b3471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "sent_test.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3707 entries, 0 to 3706\n",
            "Data columns (total 3 columns):\n",
            "Text         3707 non-null object\n",
            "Score        3707 non-null int64\n",
            "Sentiment    3707 non-null int64\n",
            "dtypes: int64(2), object(1)\n",
            "memory usage: 87.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUmvtE2GeTDb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sent_train['src'] = 0\n",
        "sent_test['src'] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfTBazT1f84X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "src = pd.concat([sent_train,sent_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX0gmDeZgADn",
        "colab_type": "code",
        "outputId": "24d159ab-90a5-4237-a2e4-c67d31dd7d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "src.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Score</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>src</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I got a free sample of these once, and now--we...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I used to get this Tea when I lived in Washing...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>This is my all time favorite 'grab and go' sna...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>This flavor is very good and unexpected.  The ...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>thrilled to have this assortment as i got the ...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text  Score  Sentiment  src\n",
              "0  I got a free sample of these once, and now--we...      5          1    0\n",
              "1  I used to get this Tea when I lived in Washing...      4          1    0\n",
              "2  This is my all time favorite 'grab and go' sna...      5          1    0\n",
              "3  This flavor is very good and unexpected.  The ...      4          1    0\n",
              "4  thrilled to have this assortment as i got the ...      4          1    0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHbENA3WgFJ4",
        "colab_type": "code",
        "outputId": "85becf14-e8b5-4e5d-893e-43bb3093d852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "src['Sentiment'].unique()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOUbsGN8g63V",
        "colab_type": "code",
        "outputId": "a081eed4-cfe0-47e5-abb2-ed82a028ed90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        }
      },
      "source": [
        "import unicodedata\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_special_characters(text, remove_digits=False):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
        "    text = re.sub(pattern, '', text)\n",
        "    return text\n",
        "  \n",
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "import contractions\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "def pos_tag_wordnet(tagged_tokens):\n",
        "    tag_map = {'j': wordnet.ADJ, 'v': wordnet.VERB, 'n': wordnet.NOUN, 'r': wordnet.ADV}\n",
        "    new_tagged_tokens = [(word, tag_map.get(tag[0].lower(), wordnet.NOUN))\n",
        "                            for word, tag in tagged_tokens]\n",
        "    return new_tagged_tokens\n",
        "\n",
        "def wordnet_lemmatize_text(text):\n",
        "    tagged_tokens = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    wordnet_tokens = pos_tag_wordnet(tagged_tokens)\n",
        "    lemmatized_text = ' '.join(wnl.lemmatize(word, tag) for word, tag in wordnet_tokens)\n",
        "    return lemmatized_text\n",
        "  \n",
        "def remove_stopwords(text, is_lower_case=False, stopwords=None):\n",
        "    if not stopwords:\n",
        "        stopwords = nltk.corpus.stopwords.words('english')\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    \n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    \n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/2a/ba0a3812e2a1de2cc4ee0ded0bdb750a7cef1631c13c78a4fc4ab042adec/contractions-0.0.21-py2.py3-none-any.whl\n",
            "Installing collected packages: contractions\n",
            "Successfully installed contractions-0.0.21\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting Unidecode (from textsearch)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 4.9MB/s \n",
            "\u001b[?25hCollecting pyahocorasick (from textsearch)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 46.7MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp36-cp36m-linux_x86_64.whl size=81708 sha256=3209c8a12ce12ef194d94a089ede0e2bf65a1cecdbbc19a4f8d50d7a9cc594d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: Unidecode, pyahocorasick, textsearch\n",
            "Successfully installed Unidecode-1.1.1 pyahocorasick-1.4.0 textsearch-0.0.17\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDyVTZ82hOBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = 'Text'\n",
        "src[col] = src[col].map(lambda x: str(x).lower())\n",
        "src[col] = src[col].map(lambda x: remove_accented_chars(x))\n",
        "src[col] = src[col].map(lambda x: remove_special_characters(x))\n",
        "src[col] = src[col].map(lambda x: contractions.fix(x))\n",
        "src[col] = src[col].map(lambda x: wordnet_lemmatize_text(x))\n",
        "src[col] = src[col].map(lambda x: remove_stopwords(x,True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp5B131Dhkbe",
        "colab_type": "code",
        "outputId": "ead4c26b-80f3-44fe-e024-ec5970383bb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "src"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Score</th>\n",
              "      <th>Sentiment</th>\n",
              "      <th>src</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>get free sample nowwere subscribe save program...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>use get tea live washington state miss niece t...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>time favorite grab go snack bar leave sugar ru...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>flavor good unexpected flavor cheese garlic go...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>thrill assortment get chance choose another de...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>love flavor coffee drawback set large cup want...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>like last review mixture dog 2 chihuahua eat a...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>wonderful handy product I thrill yummy way use...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>great value sure place bag crowd transfer hard...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>like flavor kind reminds jello pudding bite th...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>great product quick shipment place could find ...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>order parent come back trip vienna rave vienna...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>natural treat favorite crew price great free s...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>good last next thing know go everyone family l...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>great alternative coffee I try vanilla nut als...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>fruit slice deliver sugar coating melt away ru...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>provide description productbr br yesterday sho...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>love dr pepper mr pip even like walmart brandd...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>mother know divinity love say nut top divinity...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>great deal delicious lowcal guiltfree treat wr...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>bbq favorite flavor popchipsbr br popchips hea...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>receive package time specify good condition he...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>whole wheat oven ready noodle could find organ...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>get fast taste like expect exactly every goji ...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>work real well since unable find maxwell decaf...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>husband surprise christmas favorite candy avai...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>stuff good seriously almost eat know people sa...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>love cereal look forward try since like variet...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>love bar flavor favorite like option subscribe...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>soft cooky vert good try oatmeal rasin soft co...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3677</th>\n",
              "      <td>product help keep mood stressful time promote ...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3678</th>\n",
              "      <td>son eat breakfast every morning seem really en...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3679</th>\n",
              "      <td>like ginger product fine merit however sushi p...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3680</th>\n",
              "      <td>excellent coffee bold taste great aroma fill e...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3681</th>\n",
              "      <td>knorr shrimp bouillon cube irreplaceable sea f...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3682</th>\n",
              "      <td>flavor disappointed buy base review waste mone...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3683</th>\n",
              "      <td>make 1st child food use pot boil water steam b...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3684</th>\n",
              "      <td>make killer sloppy joes stuff soak mix usual r...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3685</th>\n",
              "      <td>great taste great aroma love sprinkle tablespo...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3686</th>\n",
              "      <td>send french roast favorite mine coffee adverti...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3687</th>\n",
              "      <td>span classtiny length 117 minsbr br spansee vi...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3688</th>\n",
              "      <td>use product year live reputation bed relax hel...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3689</th>\n",
              "      <td>buy bc felt sinus infection creep mespring all...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3690</th>\n",
              "      <td>baby toddler love baby food taste food enjoy t...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3691</th>\n",
              "      <td>extra bold hint flavour strong solid highpoint...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3692</th>\n",
              "      <td>read direction cooky think wow go lot cooky wo...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3693</th>\n",
              "      <td>long time lover dunkin donut ground coffee hes...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3694</th>\n",
              "      <td>really good popcorn kernel pop per batch handy...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3695</th>\n",
              "      <td>love diet big red plain simple one favorite pa...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3696</th>\n",
              "      <td>try coffee day ago love like coffee strong enj...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3697</th>\n",
              "      <td>3499 12 pound save gas money tax might want ch...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3698</th>\n",
              "      <td>guess think difference bread flour base many t...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3699</th>\n",
              "      <td>murphy softcoated wheaten terrier love treat h...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3700</th>\n",
              "      <td>originally buy cheap regular parchment paper g...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3701</th>\n",
              "      <td>buy tachino chickory brew mix 2 tablespoon tac...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3702</th>\n",
              "      <td>always powerade house everything flu overly ho...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3703</th>\n",
              "      <td>try coffee half dozen quest weekend love like ...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3704</th>\n",
              "      <td>neccos exactly expect fresh neccos great candy...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3705</th>\n",
              "      <td>think right thing get 98 fat free turkey chili...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3706</th>\n",
              "      <td>cuginos chicken noodle soup pretty tasty use b...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>18532 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   Text  Score  Sentiment  src\n",
              "0     get free sample nowwere subscribe save program...      5          1    0\n",
              "1     use get tea live washington state miss niece t...      4          1    0\n",
              "2     time favorite grab go snack bar leave sugar ru...      5          1    0\n",
              "3     flavor good unexpected flavor cheese garlic go...      4          1    0\n",
              "4     thrill assortment get chance choose another de...      4          1    0\n",
              "5     love flavor coffee drawback set large cup want...      4          1    0\n",
              "6     like last review mixture dog 2 chihuahua eat a...      5          1    0\n",
              "7     wonderful handy product I thrill yummy way use...      5          1    0\n",
              "8     great value sure place bag crowd transfer hard...      5          1    0\n",
              "9     like flavor kind reminds jello pudding bite th...      4          1    0\n",
              "10    great product quick shipment place could find ...      5          1    0\n",
              "11    order parent come back trip vienna rave vienna...      4          1    0\n",
              "12    natural treat favorite crew price great free s...      5          1    0\n",
              "13    good last next thing know go everyone family l...      5          1    0\n",
              "14    great alternative coffee I try vanilla nut als...      5          1    0\n",
              "15    fruit slice deliver sugar coating melt away ru...      1          0    0\n",
              "16    provide description productbr br yesterday sho...      4          1    0\n",
              "17    love dr pepper mr pip even like walmart brandd...      1          0    0\n",
              "18    mother know divinity love say nut top divinity...      5          1    0\n",
              "19    great deal delicious lowcal guiltfree treat wr...      5          1    0\n",
              "20    bbq favorite flavor popchipsbr br popchips hea...      5          1    0\n",
              "21    receive package time specify good condition he...      5          1    0\n",
              "22    whole wheat oven ready noodle could find organ...      5          1    0\n",
              "23    get fast taste like expect exactly every goji ...      5          1    0\n",
              "24    work real well since unable find maxwell decaf...      5          1    0\n",
              "25    husband surprise christmas favorite candy avai...      5          1    0\n",
              "26    stuff good seriously almost eat know people sa...      5          1    0\n",
              "27    love cereal look forward try since like variet...      5          1    0\n",
              "28    love bar flavor favorite like option subscribe...      5          1    0\n",
              "29    soft cooky vert good try oatmeal rasin soft co...      5          1    0\n",
              "...                                                 ...    ...        ...  ...\n",
              "3677  product help keep mood stressful time promote ...      5          1    1\n",
              "3678  son eat breakfast every morning seem really en...      5          1    1\n",
              "3679  like ginger product fine merit however sushi p...      1          0    1\n",
              "3680  excellent coffee bold taste great aroma fill e...      5          1    1\n",
              "3681  knorr shrimp bouillon cube irreplaceable sea f...      5          1    1\n",
              "3682  flavor disappointed buy base review waste mone...      2          0    1\n",
              "3683  make 1st child food use pot boil water steam b...      5          1    1\n",
              "3684  make killer sloppy joes stuff soak mix usual r...      5          1    1\n",
              "3685  great taste great aroma love sprinkle tablespo...      5          1    1\n",
              "3686  send french roast favorite mine coffee adverti...      2          0    1\n",
              "3687  span classtiny length 117 minsbr br spansee vi...      4          1    1\n",
              "3688  use product year live reputation bed relax hel...      5          1    1\n",
              "3689  buy bc felt sinus infection creep mespring all...      5          1    1\n",
              "3690  baby toddler love baby food taste food enjoy t...      5          1    1\n",
              "3691  extra bold hint flavour strong solid highpoint...      4          1    1\n",
              "3692  read direction cooky think wow go lot cooky wo...      5          1    1\n",
              "3693  long time lover dunkin donut ground coffee hes...      5          1    1\n",
              "3694  really good popcorn kernel pop per batch handy...      4          1    1\n",
              "3695  love diet big red plain simple one favorite pa...      5          1    1\n",
              "3696  try coffee day ago love like coffee strong enj...      5          1    1\n",
              "3697  3499 12 pound save gas money tax might want ch...      5          1    1\n",
              "3698  guess think difference bread flour base many t...      5          1    1\n",
              "3699  murphy softcoated wheaten terrier love treat h...      5          1    1\n",
              "3700  originally buy cheap regular parchment paper g...      5          1    1\n",
              "3701  buy tachino chickory brew mix 2 tablespoon tac...      4          1    1\n",
              "3702  always powerade house everything flu overly ho...      5          1    1\n",
              "3703  try coffee half dozen quest weekend love like ...      5          1    1\n",
              "3704  neccos exactly expect fresh neccos great candy...      5          1    1\n",
              "3705  think right thing get 98 fat free turkey chili...      1          0    1\n",
              "3706  cuginos chicken noodle soup pretty tasty use b...      5          1    1\n",
              "\n",
              "[18532 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uh6h41LCl7B3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = src[src['src'] == 0]\n",
        "test = src[src['src'] == 1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zn_vJ7Etns-r",
        "colab_type": "text"
      },
      "source": [
        "**Dont use score below example use score and predicts 100%**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5G1XHyDl_pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train[['Score','src']]\n",
        "Y_train = train['Sentiment']\n",
        "X_test = test[['Score','src']]\n",
        "Y_test = test['Sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSt3QhDxmqTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn import feature_selection\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpuZxawEk-s9",
        "colab_type": "code",
        "outputId": "de77edbb-36de-4fbb-a1a1-5538697a32f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "from sklearn import  linear_model\n",
        "mdl = linear_model.LogisticRegressionCV()\n",
        "mdl.fit(X_train,Y_train)\n",
        "Y_pred=mdl.predict(X_test)\n",
        "print(metrics.confusion_matrix(Y_test,Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[[ 592    0]\n",
            " [   0 3115]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       592\n",
            "           1       1.00      1.00      1.00      3115\n",
            "\n",
            "    accuracy                           1.00      3707\n",
            "   macro avg       1.00      1.00      1.00      3707\n",
            "weighted avg       1.00      1.00      1.00      3707\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV_F9lDCn7PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = train['Text']\n",
        "Y_train = train['Sentiment']\n",
        "X_test = test['Text']\n",
        "Y_test = test['Sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NW7K7YA8l3WM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from xgboost import XGBClassifier\n",
        "#grid search result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qsikZM0jiT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1,2),min_df=0,max_df=1.0,use_idf=True, sublinear_tf=True) \n",
        "counts = vectorizer.fit_transform(X_train)\n",
        "vocab = vectorizer.vocabulary_\n",
        "classifier = SGDClassifier(alpha=1e-05,max_iter=50,penalty='elasticnet')\n",
        "targets = Y_train\n",
        "classifier = classifier.fit(counts, targets)\n",
        "example_counts = vectorizer.transform(X_test)\n",
        "Y_pred = classifier.predict(example_counts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFKVjCVboINj",
        "colab_type": "code",
        "outputId": "3f476dcf-aa93-4768-c22f-cc44115e5ab5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "print(metrics.confusion_matrix(Y_test,Y_pred))\n",
        "print(classification_report(Y_test, Y_pred))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 374  218]\n",
            " [  89 3026]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.63      0.71       592\n",
            "           1       0.93      0.97      0.95      3115\n",
            "\n",
            "    accuracy                           0.92      3707\n",
            "   macro avg       0.87      0.80      0.83      3707\n",
            "weighted avg       0.91      0.92      0.91      3707\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU_-66MuqmKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp35StQaqHdb",
        "colab_type": "code",
        "outputId": "61411787-d0e3-41b0-ad79-18a30444be87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "for mdl in [ensemble.AdaBoostClassifier(),tree.DecisionTreeClassifier(),XGBClassifier(),naive_bayes.BernoulliNB()]:\n",
        "   classifier = mdl  \n",
        "   targets = Y_train\n",
        "   classifier = classifier.fit(counts, targets)\n",
        "   example_counts = vectorizer.transform(X_test)\n",
        "   Y_pred = classifier.predict(example_counts)\n",
        "   print(mdl.__class__.__name__) \n",
        "   print(metrics.confusion_matrix(Y_test,Y_pred))\n",
        "   print(classification_report(Y_test, Y_pred))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AdaBoostClassifier\n",
            "[[ 230  362]\n",
            " [  91 3024]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.39      0.50       592\n",
            "           1       0.89      0.97      0.93      3115\n",
            "\n",
            "    accuracy                           0.88      3707\n",
            "   macro avg       0.80      0.68      0.72      3707\n",
            "weighted avg       0.86      0.88      0.86      3707\n",
            "\n",
            "DecisionTreeClassifier\n",
            "[[ 295  297]\n",
            " [ 370 2745]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.44      0.50      0.47       592\n",
            "           1       0.90      0.88      0.89      3115\n",
            "\n",
            "    accuracy                           0.82      3707\n",
            "   macro avg       0.67      0.69      0.68      3707\n",
            "weighted avg       0.83      0.82      0.82      3707\n",
            "\n",
            "XGBClassifier\n",
            "[[ 127  465]\n",
            " [  14 3101]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.21      0.35       592\n",
            "           1       0.87      1.00      0.93      3115\n",
            "\n",
            "    accuracy                           0.87      3707\n",
            "   macro avg       0.89      0.61      0.64      3707\n",
            "weighted avg       0.87      0.87      0.84      3707\n",
            "\n",
            "BernoulliNB\n",
            "[[  19  573]\n",
            " [   5 3110]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.03      0.06       592\n",
            "           1       0.84      1.00      0.91      3115\n",
            "\n",
            "    accuracy                           0.84      3707\n",
            "   macro avg       0.82      0.52      0.49      3707\n",
            "weighted avg       0.84      0.84      0.78      3707\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGNNYO_Xjoes",
        "colab_type": "text"
      },
      "source": [
        "**Word2Vec Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgB0_mEm2TSa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9cd73800-8b58-423f-df8e-e9ca6a3281c1"
      },
      "source": [
        "import gensim\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-20 18:05:00,660 : INFO : 'pattern' package not found; tag filters are not available for English\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poeZ7pW92WJb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = LabelEncoder()\n",
        "# tokenize train reviews & encode train labels\n",
        "tokenized_train = [nltk.word_tokenize(text)\n",
        "                       for text in X_train]\n",
        "y_train = le.fit_transform(Y_train)\n",
        "# tokenize test reviews & encode test labels\n",
        "tokenized_test = [nltk.word_tokenize(text)\n",
        "                       for text in X_test]\n",
        "y_test = le.fit_transform(Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OetIOhOA2LZh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dqv55PEh57H",
        "colab_type": "code",
        "outputId": "d6262db5-4656-4a9a-dedd-f3d4794c803c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "# build word2vec model\n",
        "w2v_num_features = 500\n",
        "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=150,\n",
        "                                   min_count=10, workers=4, iter=5)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-20 18:32:57,523 : INFO : collecting all words and their counts\n",
            "2019-09-20 18:32:57,525 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-09-20 18:32:57,602 : INFO : PROGRESS: at sentence #10000, processed 399863 words, keeping 23049 word types\n",
            "2019-09-20 18:32:57,639 : INFO : collected 28994 word types from a corpus of 593315 raw words and 14825 sentences\n",
            "2019-09-20 18:32:57,640 : INFO : Loading a fresh vocabulary\n",
            "2019-09-20 18:32:57,661 : INFO : effective_min_count=10 retains 4246 unique words (14% of original 28994, drops 24748)\n",
            "2019-09-20 18:32:57,662 : INFO : effective_min_count=10 leaves 544362 word corpus (91% of original 593315, drops 48953)\n",
            "2019-09-20 18:32:57,680 : INFO : deleting the raw counts dictionary of 28994 items\n",
            "2019-09-20 18:32:57,682 : INFO : sample=0.001 downsamples 58 most-common words\n",
            "2019-09-20 18:32:57,683 : INFO : downsampling leaves estimated 478294 word corpus (87.9% of prior 544362)\n",
            "2019-09-20 18:32:57,698 : INFO : estimated required memory for 4246 words and 500 dimensions: 19107000 bytes\n",
            "2019-09-20 18:32:57,699 : INFO : resetting layer weights\n",
            "2019-09-20 18:32:57,773 : INFO : training model with 4 workers on 4246 vocabulary and 500 features, using sg=0 hs=0 sample=0.001 negative=5 window=150\n",
            "2019-09-20 18:32:58,879 : INFO : EPOCH 1 - PROGRESS: at 22.12% examples, 95353 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:32:59,884 : INFO : EPOCH 1 - PROGRESS: at 45.98% examples, 103374 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:00,911 : INFO : EPOCH 1 - PROGRESS: at 67.14% examples, 102513 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:01,952 : INFO : EPOCH 1 - PROGRESS: at 90.82% examples, 103846 words/s, in_qsize 6, out_qsize 0\n",
            "2019-09-20 18:33:02,093 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-20 18:33:02,223 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-20 18:33:02,242 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-20 18:33:02,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-20 18:33:02,249 : INFO : EPOCH - 1 : training on 593315 raw words (478364 effective words) took 4.5s, 107020 effective words/s\n",
            "2019-09-20 18:33:03,415 : INFO : EPOCH 2 - PROGRESS: at 23.97% examples, 97156 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:04,467 : INFO : EPOCH 2 - PROGRESS: at 47.82% examples, 101865 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:05,538 : INFO : EPOCH 2 - PROGRESS: at 70.38% examples, 102777 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:06,609 : INFO : EPOCH 2 - PROGRESS: at 95.87% examples, 105085 words/s, in_qsize 3, out_qsize 1\n",
            "2019-09-20 18:33:06,612 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-20 18:33:06,617 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-20 18:33:06,669 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-20 18:33:06,677 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-20 18:33:06,678 : INFO : EPOCH - 2 : training on 593315 raw words (478236 effective words) took 4.4s, 108165 effective words/s\n",
            "2019-09-20 18:33:07,801 : INFO : EPOCH 3 - PROGRESS: at 22.12% examples, 93880 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:08,804 : INFO : EPOCH 3 - PROGRESS: at 45.98% examples, 102554 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:09,858 : INFO : EPOCH 3 - PROGRESS: at 68.72% examples, 103805 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:10,938 : INFO : EPOCH 3 - PROGRESS: at 92.39% examples, 103820 words/s, in_qsize 5, out_qsize 0\n",
            "2019-09-20 18:33:10,979 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-20 18:33:11,060 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-20 18:33:11,114 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-20 18:33:11,138 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-20 18:33:11,139 : INFO : EPOCH - 3 : training on 593315 raw words (478424 effective words) took 4.5s, 107451 effective words/s\n",
            "2019-09-20 18:33:12,146 : INFO : EPOCH 4 - PROGRESS: at 18.92% examples, 88153 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:13,165 : INFO : EPOCH 4 - PROGRESS: at 44.18% examples, 103493 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:14,260 : INFO : EPOCH 4 - PROGRESS: at 65.56% examples, 100482 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:15,444 : INFO : EPOCH 4 - PROGRESS: at 92.60% examples, 102623 words/s, in_qsize 5, out_qsize 0\n",
            "2019-09-20 18:33:15,466 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-20 18:33:15,530 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-20 18:33:15,584 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-20 18:33:15,598 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-20 18:33:15,600 : INFO : EPOCH - 4 : training on 593315 raw words (478192 effective words) took 4.5s, 107356 effective words/s\n",
            "2019-09-20 18:33:16,738 : INFO : EPOCH 5 - PROGRESS: at 22.12% examples, 92400 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:17,771 : INFO : EPOCH 5 - PROGRESS: at 47.48% examples, 104082 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:18,798 : INFO : EPOCH 5 - PROGRESS: at 68.72% examples, 103215 words/s, in_qsize 7, out_qsize 0\n",
            "2019-09-20 18:33:19,877 : INFO : EPOCH 5 - PROGRESS: at 92.39% examples, 103404 words/s, in_qsize 5, out_qsize 0\n",
            "2019-09-20 18:33:19,914 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-09-20 18:33:20,043 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-09-20 18:33:20,067 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-09-20 18:33:20,073 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-09-20 18:33:20,074 : INFO : EPOCH - 5 : training on 593315 raw words (478609 effective words) took 4.5s, 107154 effective words/s\n",
            "2019-09-20 18:33:20,075 : INFO : training on a 2966575 raw words (2391825 effective words) took 22.3s, 107253 effective words/s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 44 s, sys: 67.4 ms, total: 44 s\n",
            "Wall time: 22.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA46jmEDepq2",
        "colab_type": "code",
        "outputId": "d00b4c28-8d27-4554-c17a-0c978fe7ca62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w2v_model"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7f8379d43be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gBHN7-J4r34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
        "    vocabulary = set(model.wv.index2word)\n",
        "    \n",
        "    def average_word_vectors(words, model, vocabulary, num_features):\n",
        "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
        "        nwords = 0.\n",
        "        \n",
        "        for word in words:\n",
        "            if word in vocabulary: \n",
        "                nwords = nwords + 1.\n",
        "                feature_vector = np.add(feature_vector, model.wv[word])\n",
        "        if nwords:\n",
        "            feature_vector = np.divide(feature_vector, nwords)\n",
        "\n",
        "        return feature_vector\n",
        "\n",
        "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
        "                    for tokenized_sentence in corpus]\n",
        "    return np.array(features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPkLR9F64uF7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate averaged word vector features from word2vec model\n",
        "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
        "                                                     num_features=w2v_num_features)\n",
        "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
        "                                                    num_features=w2v_num_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UipZHbi649MO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5564b46f-dfc7-4a2c-a673-0c70b54f2932"
      },
      "source": [
        "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec model:> Train features shape: (14825, 500)  Test features shape: (3707, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA9FNXBI67Qe",
        "colab_type": "text"
      },
      "source": [
        "ML Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gznJRGa_6ut2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "a630a858-4474-422d-d54d-e574f965e743"
      },
      "source": [
        "classifier = SGDClassifier(alpha=1e-05,max_iter=150,penalty='elasticnet')\n",
        "classifier = classifier.fit(avg_wv_train_features, y_train)\n",
        "Y_pred = classifier.predict(avg_wv_test_features)\n",
        "print(metrics.confusion_matrix(y_test,Y_pred))\n",
        "print(classification_report(y_test, Y_pred))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 222  370]\n",
            " [ 191 2924]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.54      0.38      0.44       592\n",
            "           1       0.89      0.94      0.91      3115\n",
            "\n",
            "    accuracy                           0.85      3707\n",
            "   macro avg       0.71      0.66      0.68      3707\n",
            "weighted avg       0.83      0.85      0.84      3707\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX89Oz4-7SJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "76476186-358a-4042-d8d4-10b531be92c2"
      },
      "source": [
        "for mdl in [ensemble.AdaBoostClassifier(n_estimators=150),tree.DecisionTreeClassifier(),XGBClassifier(n_estimator=200),naive_bayes.BernoulliNB()]:\n",
        "   classifier = mdl  \n",
        "   classifier = classifier.fit(avg_wv_train_features, y_train)\n",
        "   Y_pred = classifier.predict(avg_wv_test_features)\n",
        "   print(mdl.__class__.__name__) \n",
        "   print(metrics.confusion_matrix(y_test,Y_pred))\n",
        "   print(classification_report(y_test, Y_pred))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AdaBoostClassifier\n",
            "[[ 222  370]\n",
            " [ 161 2954]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.58      0.38      0.46       592\n",
            "           1       0.89      0.95      0.92      3115\n",
            "\n",
            "    accuracy                           0.86      3707\n",
            "   macro avg       0.73      0.66      0.69      3707\n",
            "weighted avg       0.84      0.86      0.84      3707\n",
            "\n",
            "DecisionTreeClassifier\n",
            "[[ 249  343]\n",
            " [ 385 2730]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.39      0.42      0.41       592\n",
            "           1       0.89      0.88      0.88      3115\n",
            "\n",
            "    accuracy                           0.80      3707\n",
            "   macro avg       0.64      0.65      0.64      3707\n",
            "weighted avg       0.81      0.80      0.81      3707\n",
            "\n",
            "XGBClassifier\n",
            "[[ 198  394]\n",
            " [ 115 3000]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.33      0.44       592\n",
            "           1       0.88      0.96      0.92      3115\n",
            "\n",
            "    accuracy                           0.86      3707\n",
            "   macro avg       0.76      0.65      0.68      3707\n",
            "weighted avg       0.84      0.86      0.84      3707\n",
            "\n",
            "BernoulliNB\n",
            "[[ 451  141]\n",
            " [ 892 2223]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.34      0.76      0.47       592\n",
            "           1       0.94      0.71      0.81      3115\n",
            "\n",
            "    accuracy                           0.72      3707\n",
            "   macro avg       0.64      0.74      0.64      3707\n",
            "weighted avg       0.84      0.72      0.76      3707\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQEblqVT7sEl",
        "colab_type": "text"
      },
      "source": [
        "FNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXvV_8da7iOm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_deepnn_architecture(num_input_features):\n",
        "    dnn_model = Sequential()\n",
        "    dnn_model.add(Dense(512, input_shape=(num_input_features,)))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(256))\n",
        "    dnn_model.add(Activation('relu'))\n",
        "    dnn_model.add(Dropout(0.2))\n",
        "    \n",
        "    dnn_model.add(Dense(1))\n",
        "    dnn_model.add(Activation('sigmoid'))\n",
        "\n",
        "    dnn_model.compile(loss='binary_crossentropy', optimizer='adam',                 \n",
        "                      metrics=['accuracy'])\n",
        "    return dnn_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBze8pbB7xGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMKTg01t71Z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "708fe77b-affd-43e7-bd38-b191fc5b0f8e"
      },
      "source": [
        "w2v_dnn.summary()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 512)               256512    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1)                 257       \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 453,889\n",
            "Trainable params: 453,889\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqu8bRMM75c-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "82b5c651-18d5-427b-ea0a-35074da1ddad"
      },
      "source": [
        "batch_size = 100\n",
        "w2v_dnn.fit(avg_wv_train_features, y_train, epochs=10, batch_size=batch_size, \n",
        "            shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 13342 samples, validate on 1483 samples\n",
            "Epoch 1/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.2893 - acc: 0.8759 - val_loss: 0.3001 - val_acc: 0.8651\n",
            "Epoch 2/50\n",
            "13342/13342 [==============================] - 2s 118us/sample - loss: 0.2833 - acc: 0.8802 - val_loss: 0.2979 - val_acc: 0.8685\n",
            "Epoch 3/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.2841 - acc: 0.8786 - val_loss: 0.2993 - val_acc: 0.8672\n",
            "Epoch 4/50\n",
            "13342/13342 [==============================] - 2s 115us/sample - loss: 0.2790 - acc: 0.8804 - val_loss: 0.3105 - val_acc: 0.8712\n",
            "Epoch 5/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.2761 - acc: 0.8823 - val_loss: 0.3142 - val_acc: 0.8665\n",
            "Epoch 6/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.2715 - acc: 0.8814 - val_loss: 0.3154 - val_acc: 0.8665\n",
            "Epoch 7/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.2664 - acc: 0.8861 - val_loss: 0.3026 - val_acc: 0.8753\n",
            "Epoch 8/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.2685 - acc: 0.8859 - val_loss: 0.3093 - val_acc: 0.8699\n",
            "Epoch 9/50\n",
            "13342/13342 [==============================] - 2s 115us/sample - loss: 0.2633 - acc: 0.8871 - val_loss: 0.2987 - val_acc: 0.8753\n",
            "Epoch 10/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.2563 - acc: 0.8903 - val_loss: 0.3142 - val_acc: 0.8726\n",
            "Epoch 11/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.2566 - acc: 0.8913 - val_loss: 0.3053 - val_acc: 0.8746\n",
            "Epoch 12/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.2517 - acc: 0.8939 - val_loss: 0.3142 - val_acc: 0.8692\n",
            "Epoch 13/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.2446 - acc: 0.8948 - val_loss: 0.3181 - val_acc: 0.8692\n",
            "Epoch 14/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.2395 - acc: 0.8951 - val_loss: 0.3364 - val_acc: 0.8604\n",
            "Epoch 15/50\n",
            "13342/13342 [==============================] - 2s 114us/sample - loss: 0.2379 - acc: 0.8969 - val_loss: 0.3187 - val_acc: 0.8746\n",
            "Epoch 16/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.2323 - acc: 0.9010 - val_loss: 0.3228 - val_acc: 0.8806\n",
            "Epoch 17/50\n",
            "13342/13342 [==============================] - 2s 115us/sample - loss: 0.2279 - acc: 0.8991 - val_loss: 0.3426 - val_acc: 0.8719\n",
            "Epoch 18/50\n",
            "13342/13342 [==============================] - 2s 118us/sample - loss: 0.2249 - acc: 0.9026 - val_loss: 0.3347 - val_acc: 0.8678\n",
            "Epoch 19/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.2158 - acc: 0.9076 - val_loss: 0.3499 - val_acc: 0.8658\n",
            "Epoch 20/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.2144 - acc: 0.9073 - val_loss: 0.3508 - val_acc: 0.8537\n",
            "Epoch 21/50\n",
            "13342/13342 [==============================] - 2s 114us/sample - loss: 0.2108 - acc: 0.9083 - val_loss: 0.3581 - val_acc: 0.8631\n",
            "Epoch 22/50\n",
            "13342/13342 [==============================] - 2s 115us/sample - loss: 0.2046 - acc: 0.9146 - val_loss: 0.3723 - val_acc: 0.8665\n",
            "Epoch 23/50\n",
            "13342/13342 [==============================] - 2s 115us/sample - loss: 0.2040 - acc: 0.9117 - val_loss: 0.3689 - val_acc: 0.8618\n",
            "Epoch 24/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.1970 - acc: 0.9175 - val_loss: 0.3749 - val_acc: 0.8591\n",
            "Epoch 25/50\n",
            "13342/13342 [==============================] - 2s 115us/sample - loss: 0.1892 - acc: 0.9194 - val_loss: 0.4109 - val_acc: 0.8618\n",
            "Epoch 26/50\n",
            "13342/13342 [==============================] - 2s 115us/sample - loss: 0.1902 - acc: 0.9182 - val_loss: 0.3977 - val_acc: 0.8537\n",
            "Epoch 27/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.1867 - acc: 0.9204 - val_loss: 0.3988 - val_acc: 0.8699\n",
            "Epoch 28/50\n",
            "13342/13342 [==============================] - 2s 118us/sample - loss: 0.1820 - acc: 0.9240 - val_loss: 0.3889 - val_acc: 0.8517\n",
            "Epoch 29/50\n",
            "13342/13342 [==============================] - 2s 115us/sample - loss: 0.1797 - acc: 0.9265 - val_loss: 0.4179 - val_acc: 0.8584\n",
            "Epoch 30/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.1746 - acc: 0.9277 - val_loss: 0.4179 - val_acc: 0.8537\n",
            "Epoch 31/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.1665 - acc: 0.9291 - val_loss: 0.4600 - val_acc: 0.8483\n",
            "Epoch 32/50\n",
            "13342/13342 [==============================] - 2s 115us/sample - loss: 0.1711 - acc: 0.9274 - val_loss: 0.4345 - val_acc: 0.8483\n",
            "Epoch 33/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.1578 - acc: 0.9331 - val_loss: 0.4415 - val_acc: 0.8611\n",
            "Epoch 34/50\n",
            "13342/13342 [==============================] - 2s 119us/sample - loss: 0.1591 - acc: 0.9322 - val_loss: 0.4366 - val_acc: 0.8611\n",
            "Epoch 35/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.1514 - acc: 0.9366 - val_loss: 0.4660 - val_acc: 0.8557\n",
            "Epoch 36/50\n",
            "13342/13342 [==============================] - 2s 115us/sample - loss: 0.1525 - acc: 0.9322 - val_loss: 0.4777 - val_acc: 0.8543\n",
            "Epoch 37/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.1519 - acc: 0.9370 - val_loss: 0.4509 - val_acc: 0.8584\n",
            "Epoch 38/50\n",
            "13342/13342 [==============================] - 2s 119us/sample - loss: 0.1389 - acc: 0.9424 - val_loss: 0.4744 - val_acc: 0.8597\n",
            "Epoch 39/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.1416 - acc: 0.9402 - val_loss: 0.4861 - val_acc: 0.8543\n",
            "Epoch 40/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.1373 - acc: 0.9431 - val_loss: 0.4975 - val_acc: 0.8584\n",
            "Epoch 41/50\n",
            "13342/13342 [==============================] - 2s 115us/sample - loss: 0.1384 - acc: 0.9439 - val_loss: 0.4986 - val_acc: 0.8591\n",
            "Epoch 42/50\n",
            "13342/13342 [==============================] - 2s 114us/sample - loss: 0.1373 - acc: 0.9430 - val_loss: 0.4917 - val_acc: 0.8530\n",
            "Epoch 43/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.1338 - acc: 0.9461 - val_loss: 0.4928 - val_acc: 0.8570\n",
            "Epoch 44/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.1308 - acc: 0.9478 - val_loss: 0.5036 - val_acc: 0.8543\n",
            "Epoch 45/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.1274 - acc: 0.9480 - val_loss: 0.5208 - val_acc: 0.8543\n",
            "Epoch 46/50\n",
            "13342/13342 [==============================] - 2s 116us/sample - loss: 0.1287 - acc: 0.9481 - val_loss: 0.5313 - val_acc: 0.8557\n",
            "Epoch 47/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.1295 - acc: 0.9475 - val_loss: 0.5202 - val_acc: 0.8530\n",
            "Epoch 48/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.1176 - acc: 0.9530 - val_loss: 0.5368 - val_acc: 0.8611\n",
            "Epoch 49/50\n",
            "13342/13342 [==============================] - 2s 117us/sample - loss: 0.1266 - acc: 0.9503 - val_loss: 0.5035 - val_acc: 0.8490\n",
            "Epoch 50/50\n",
            "13342/13342 [==============================] - 2s 118us/sample - loss: 0.1199 - acc: 0.9496 - val_loss: 0.5375 - val_acc: 0.8550\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8374537898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgi0XN-K8ADw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "d7b322f9-ae83-4aa7-e7b1-69896c792d47"
      },
      "source": [
        "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
        "predictions = le.inverse_transform(y_pred)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:273: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihbyLVTW8DhL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "f6cc269e-a625-4a98-b517-16377094738f"
      },
      "source": [
        "print(classification_report(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.57      0.42      0.48       592\n",
            "           1       0.90      0.94      0.92      3115\n",
            "\n",
            "    accuracy                           0.86      3707\n",
            "   macro avg       0.73      0.68      0.70      3707\n",
            "weighted avg       0.84      0.86      0.85      3707\n",
            "\n",
            "[[ 249  343]\n",
            " [ 189 2926]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtMCn_-X-ESH",
        "colab_type": "text"
      },
      "source": [
        "CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_Ar0MLt_SLV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffec6b08-b5ab-47b0-c629-188c52de5409"
      },
      "source": [
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from keras.layers import BatchNormalization"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3jYmzKDCtfN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "73922c7f-e4e0-45d2-8d0b-40c9a64ff9bf"
      },
      "source": [
        "t = Tokenizer(oov_token='<UNK>')\n",
        "# fit the tokenizer on the documents\n",
        "t.fit_on_texts(X_train)\n",
        "t.word_index['<PAD>'] = 0\n",
        "max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), t.word_index['<UNK>']"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('solubles', 28995), ('<PAD>', 0), 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN7AMHz3DYhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_sequences = t.texts_to_sequences(X_train)\n",
        "test_sequences = t.texts_to_sequences(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9vzderIC2tG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b08ac2a8-65d8-4943-89cf-910d73d9f5c8"
      },
      "source": [
        "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
        "print(\"Number of Documents={}\".format(t.document_count))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size=28996\n",
            "Number of Documents=14825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQwu8aR5DrVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CzsdY8vDxjG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2a2ab637-6c8b-4fa4-b95a-6312261371f1"
      },
      "source": [
        "# pad dataset to a maximum review length in words\n",
        "C_train = sequence.pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "C_test = sequence.pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "C_train.shape, C_test.shape"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14825, 1000), (3707, 1000))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqzP7f0pEE4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "le = LabelEncoder()\n",
        "num_classes=2 # positive -> 1, negative -> 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDKdi30h8ihX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBED_SIZE = 300\n",
        "EPOCHS=3\n",
        "BATCH_SIZE=128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nK0zOqGA1aN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "c78cdd41-ca66-4b82-ae07-611d1450d5e0"
      },
      "source": [
        "# create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(len(t.word_index), EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH))\n",
        "model.add(Conv1D(filters=128, kernel_size=3, padding='same', activation='elu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='elu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='elu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='elu')) \n",
        "model.add(Dense(128, activation='elu'))\n",
        "model.add(Dense(64, activation='elu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_11 (Embedding)     (None, 1000, 300)         8698800   \n",
            "_________________________________________________________________\n",
            "conv1d_33 (Conv1D)           (None, 1000, 128)         115328    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_33 (MaxPooling (None, 500, 128)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_34 (Conv1D)           (None, 500, 64)           24640     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_34 (MaxPooling (None, 250, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_35 (Conv1D)           (None, 250, 32)           6176      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_35 (MaxPooling (None, 125, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 4000)              0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 256)               1024256   \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 1)                 65        \n",
            "=================================================================\n",
            "Total params: 9,910,417\n",
            "Trainable params: 9,910,417\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XraXsxetBOR9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "cf079944-ff1d-4761-da16-7ffd7c459c00"
      },
      "source": [
        "%%time\n",
        "# Fit the model\n",
        "model.fit(C_train, y_train, \n",
        "          validation_split=0.1,\n",
        "          epochs=EPOCHS, \n",
        "          batch_size=BATCH_SIZE, \n",
        "          verbose=1)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 13342 samples, validate on 1483 samples\n",
            "Epoch 1/3\n",
            "13342/13342 [==============================] - 262s 20ms/sample - loss: 0.4804 - acc: 0.8326 - val_loss: 0.3834 - val_acc: 0.8496\n",
            "Epoch 2/3\n",
            "13342/13342 [==============================] - 263s 20ms/sample - loss: 0.2221 - acc: 0.9098 - val_loss: 0.2163 - val_acc: 0.9117\n",
            "Epoch 3/3\n",
            "13342/13342 [==============================] - 263s 20ms/sample - loss: 0.0376 - acc: 0.9878 - val_loss: 0.3026 - val_acc: 0.9002\n",
            "CPU times: user 25min 41s, sys: 13.8 s, total: 25min 55s\n",
            "Wall time: 13min 9s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f8370665f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNnyxl2MHU1-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model.predict_classes(C_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kfSIxEOBgtq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a3841a3c-0518-4ba9-ce18-76001a704b19"
      },
      "source": [
        "print(classification_report(y_test, predictions))\n",
        "print(confusion_matrix(y_test, predictions))"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.66      0.65       592\n",
            "           1       0.94      0.93      0.93      3115\n",
            "\n",
            "    accuracy                           0.89      3707\n",
            "   macro avg       0.79      0.80      0.79      3707\n",
            "weighted avg       0.89      0.89      0.89      3707\n",
            "\n",
            "[[ 392  200]\n",
            " [ 216 2899]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYD-1MvIUvT1",
        "colab_type": "text"
      },
      "source": [
        "LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTYIziPfVI3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSIXCZ6FUtVv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "cb21826d-0efa-465e-897a-29bda7f04f27"
      },
      "source": [
        "EMBEDDING_DIM = 300 # dimension for dense embeddings for each token\n",
        "LSTM_DIM = 128 # total LSTM units\n",
        "\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=len(t.word_index), output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
        "model.add(tf.keras.layers.SpatialDropout1D(0.1))\n",
        "model.add(tf.keras.layers.CuDNNLSTM(LSTM_DIM, return_sequences=False))\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_12 (Embedding)     (None, 1000, 300)         8698800   \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d (SpatialDr (None, 1000, 300)         0         \n",
            "_________________________________________________________________\n",
            "cu_dnnlstm (CuDNNLSTM)       (None, 128)               220160    \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 1)                 257       \n",
            "=================================================================\n",
            "Total params: 8,952,241\n",
            "Trainable params: 8,952,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpEJ_vbHV7Vr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "826302ba-6361-44bf-e826-c16095c6734d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0QvUXXIU9fA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "outputId": "ea0be850-c70b-4ef9-f78e-247e3729cd4c"
      },
      "source": [
        "batch_size = 100\n",
        "model.fit(C_train, y_train, epochs=2, batch_size=batch_size, \n",
        "          shuffle=True, validation_split=0.1, verbose=1)"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 13342 samples, validate on 1483 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNN' used by {{node cu_dnnlstm/CudnnRNN}}with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", is_training=true, seed2=0]\nRegistered devices: [CPU, XLA_CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[cu_dnnlstm/CudnnRNN]]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-152-c4b72a37cd60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model.fit(C_train, y_train, epochs=2, batch_size=batch_size, \n\u001b[0;32m----> 3\u001b[0;31m           shuffle=True, validation_split=0.1, verbose=1)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m           steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;31m# Setup work for each epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0mepoch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'metrics'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;31m# Reset the state of loss metric wrappers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \"\"\"\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3069\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3071\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    460\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    877\u001b[0m     \u001b[0;31m# marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m     is_initialized = session.run(\n\u001b[0;32m--> 879\u001b[0;31m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    880\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1368\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1370\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: No OpKernel was registered to support Op 'CudnnRNN' used by node cu_dnnlstm/CudnnRNN (defined at <ipython-input-151-190929de1954>:7) with these attrs: [seed=0, dropout=0, T=DT_FLOAT, input_mode=\"linear_input\", direction=\"unidirectional\", rnn_mode=\"lstm\", is_training=true, seed2=0]\nRegistered devices: [CPU, XLA_CPU]\nRegistered kernels:\n  <no registered kernels>\n\n\t [[cu_dnnlstm/CudnnRNN]]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbMbE95IVcRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}